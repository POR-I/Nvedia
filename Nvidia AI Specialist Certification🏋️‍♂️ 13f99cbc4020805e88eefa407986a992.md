# Nvidia AI Specialist Certification🏋️‍♂️

# 프로젝트 개요(**Overview of the Project)**

- **배경 정보 소개(Backgrounds of project)**
- **프로젝트의 전반적 내용(General description of the current project)**
- **제안하는 프로젝트의 강점
(Proposed idea for enhancements to the project)**
- **프로젝트의 가치(Value and significance of this project)**
- **직면하고 있는 한계(Current limitations)**
- **결과물(Results)**

# **🏋️‍♂️**  프로젝트 주제 (Title) **🏋️‍♂️**

## **YOLOV5를 이용한 피트니스 소도구 판별**

---

## **Fitness Equipment Identification Using YOLOV5**

# 배경 정보

   **Background information**

---

<aside>
**이 프로젝트를 통해 고객은 피트니스 소품의 정확한 이름과 용도를 파악할 수 있습니다**

**사물을 다르게 인식하는 데 도움이 되는 YOLOv5 기반 사물 인식 모델**

**소품을 사용하여 실시간으로 소품을 인식하고 소품의 이름을 알려주는 시스템**

**건축에 관한 것입니다. 고객은 카메라 앞에 다양한 소품을 가져옵니다**

**대면하여 시스템이 이를 인식하고 도구의 이름과 용도를 즉시 알려줍니다**

**운동 도구와 운동 효과에 대해 자세히 알아보기 위해 고객에게 제공합니다**

**비율을 높이고 올바른 도구 사용 방법을 배울 수 있습니다.**

</aside>

---

This project aims to build a system that uses a YOLOv5-based object recognition model to identify fitness accessories in real-time and provide their names, helping customers quickly recognize the correct names and purposes of these items. When customers present various accessories in front of the camera, the system recognizes them and immediately provides the name and purpose of each tool. Through this, customers can learn more about exercise equipment, increase their workout efficiency, and learn the correct ways to use the tools.

---

# 프로젝트의 전반적 설명

  **General description of the current project**

---

<aside>
🏋️ **운동 중에는 다양한 소도구들이 사용되지만, 많은 고객들이 이 소도구들의 이름을 정확히 인식하지 못하는 경우가 많다. 케틀벨, 요가 블록, 라크로스 볼, 폼 롤러 등 각기 다른 소도구들은 각각의 운동에 맞는 중요한 역할을 한다. 하지만 이 도구들의 정확한 명칭을 모르거나 혼동하는 경우가 자주 발생한다. 이로 인해 고객들은 운동 중에 어떤 도구를 사용해야 하는지, 어떻게 활용해야 하는 지에 대한 혼란을 겪고 있다. 또한, 이름을 몰라 제대로 된 정보를 찾기 위해 추가적인 검색을 해야 하거나, 올바른 도구를 선택하는 데 시간이 낭비되곤 하기에 이를 방지하여 소도구를 실시간으로 인식하고 트레이닝에 실용적인 정보를 제공한다.**

</aside>

---

Various accessories are used during exercise, but many customers often fail to accurately recognize the names of these accessories. Different tools such as kettlebells, yoga blocks, lacrosse balls, and foam rollers each play important roles in specific exercises. However, people frequently don't know or confuse the exact names of these tools. As a result, customers experience confusion about which tools to use during workouts and how to utilize them. Additionally, not knowing the names often leads to the need for additional searches to find proper information or wasted time in selecting the right tool. To prevent this, we provide real-time recognition of accessories and practical training information.

# 제안하고 싶은 프로젝트의 강점

 **Proposed idea for enhancements to the project**     

---

---

- **효율적인 학습:** 
고객들은 직접 검색하지 않고도 실시간으로 도구에 대한 정보를 얻을 수 있어 학습 속도 증가
- **사용자 경험 향상:** 
운동 중 사용되는 도구를 바로 인식할 수 있어 더 이상 혼동 없이 운동에 집중하는 환경 제공
- **운동 효과 극대화:** 
올바른 도구를 정확하게 사용할 수 있어, 운동 효과를 극대화하고 부상 위험 감소
- **기술과 운동의 융합:** 
최신 AI 기술을 활용하여 운동과 학습을 동시에 할 수 있는 환경을 제공함으로써, 보다 스마트한 운동 경험 제공
    
    실제로 PT샵과 같은 곳은 보여지는 것과 서비스제공이 중요하기에 충분한 경쟁력과 편리함 제공(**In fact, places like PT shops offer enough competitiveness and convenience for what they see and what they serve to be important)**
    
- **트레이너와 고객 간의 소통 강화:** 
트레이너는 고객들이 도구를 올바르게 사용할 수 있도록 도와줄 수 있으며, 이 시스템을 통해 고객과 원활한 소통 통한 더 나은 피드백 제공

---

- **Efficient learning: Customers can obtain real-time information about tools without direct searching, increasing the speed of learning**
- **Enhanced user experience: Providing an environment where users can focus on exercising without confusion by instantly recognizing tools used during workouts**
- **Maximizing exercise effectiveness: Ability to use the correct tools accurately, maximizing exercise effects and reducing the risk of injury**
- **Integration of technology and exercise: Providing a smarter exercise experience by offering an environment where users can exercise and learn simultaneously using the latest AI technologyIn fact, places like PT shops provide sufficient competitiveness and convenience as the visual aspect and service provision are important**
- **Strengthening communication between trainers and clients: Trainers can help clients use tools correctly, and through this system, provide better feedback through smooth communication with clients**

---

# 프로젝트의 가치와 중요성

 **Value and significance of the project**

<aside>
🏋️ **이 프로젝트는 피트니스 소도구를 실시간으로 인식하고 그 이름과 사용법을 알려주는 AI 시스템으로 많은 고객들이 운동 도구의 이름이나 사용법을 잘 몰라 비효율적으로 학습하거나 부상을 초래할 수 있다.

이 시스템은 이를 해결하고, 고객이 도구를 빠르고 정확하게 인식해 운동 효과를 극대화하고 부상을 방지하도록 도우며 YOLOv5를 활용해 도구를 실시간으로 인식하고, 트레이너와 고객 간 소통을 원활하게 만들어 더 안전하고 효율적인 운동 환경을 제공한다.**

</aside>

---

**This project is an AI system that recognizes fitness accessories in real-time and provides their names and usage instructions. Many customers may learn inefficiently or risk injury due to lack of knowledge about the names or proper use of exercise equipment.**

**This system aims to solve this issue by helping customers quickly and accurately recognize tools, maximizing exercise effectiveness and preventing injuries. By utilizing YOLOv5 to recognize tools in real-time, it facilitates smoother communication between trainers and customers, providing a safer and more efficient exercise environment.**

# 한계점

**Current limitations**

---

<aside>
🏋️ **이 시스템은 피트니스 소도구를 실시간으로 인식하여 개인 맞춤형 운동 트래킹 정보를 제공하지만, 모든 운동을 완벽하게 개인화하는 데에는 한계가 존재한다. 소도구의 브랜드나 제조사에 따라 디자인, 색상, 크기 등이 다양하기 때문에 동일한 도구라도 인식이 어려운 경우가 발생할 수 있다. 이는 다양한 소도구의 변형된 형태나 색상 차이에 의해 완벽한 인식이 힘들다는 현실적인 도전 과제이지만.지속적인 개선을 통해 점차 더 높은 인식률을 목표로 하고 있다.**

</aside>

---

**This system provides real-time recognition of fitness accessories and personalized exercise tracking information, but there are limitations in perfectly personalizing all exercises. Due to the variety in design, color, and size depending on the brand or manufacturer of accessories, there may be cases where even identical tools are difficult to recognize. This presents a realistic challenge in achieving perfect recognition due to the varied forms and color differences of diverse accessories. However, we aim for gradually higher recognition rates through continuous improvements.**

# 문헌 고찰

 **Literature review**

---

<aside>
🏋️ 1.라이다와 카메라의 한계와 Yolov5를 활용한 객체 감지 및 추적 기술에 대한 선행 연구를 통해 이 프로젝트의 기술적 배경을 충분한 파악이 필요하다.
2.꼬깔콘에 대한 다양한 학습 데이터와 인공지능 학습: 이미지 인식 , 객체 탐지 및 분류에 대한 다양한 이해가 필요하다.

</aside>

---

- **It is essential to gain a thorough understanding of the limitations of LiDAR and cameras, as well as the technical background of object detection and tracking using YOLOv5 through prior research, to adequately grasp the technological context of this project.**
- **Diverse training data for traffic signals and artificial intelligence training: A comprehensive understanding of image recognition, object detection, and classification is necessary for this project.**

# 영상 취득 방법**(Image Acquisition Method)**:

---

- **인공지능 영상 취득 : 직접 일하는 곳에서 각종 소도구들을 촬영**
(Filmed various props at the place where I work)

[https://drive.google.com/file/d/12TG2KZcvgloyKzglvqSZP8Uu5N6jdAa2/view?usp=drive_link](https://drive.google.com/file/d/12TG2KZcvgloyKzglvqSZP8Uu5N6jdAa2/view?usp=drive_link)

**해당 영상을 이용하여 DarkLabel에서 라벨링을 통한 추출**
(Use this video to extract from DarkLabel through labeling)

# **DarkLabel을 이용한 영상 Labeling 및 Innotation**

---

- **데이터 추출: 수집한 영상을 DarkLabel 에서 이미지로 추출합니다.**
Extract the collected images from DarkLabel as images.
****

[DarkLabel2.4.zip](DarkLabel2.4.zip)

![Untitled](Untitled.png)

**Yolov5는 640 * 640 에서 지원하므로 원본영상을 해상도 변환 프로그램을 
이용하여 640 * 640 으로설정한다.**
(Since Yolov5 is supported by 640 * 640, the original image is set to 640 * 640 using a resolution conversion program.)

![뱁믹스 인코딩 설정.JPG](25d80498-33a9-49d4-acb0-a37e45c89780.png)

[[https://www.bing.com/ck/a?!&&p=17025025dbeef5d650a67a3b4a49c0218bed35529ef61e6ddbfc38d974981ab8JmltdHM9MTczMTYyODgwMA&ptn=3&ver=2&hsh=4&fclid=29425e80-95ec-62a3-2119-51b5944e63b9&psq=뱁믹스&u=a1aHR0cHM6Ly93d3cudmFwc2hpb24uY29tL3ZhcHNoaW9uMy9kb3dubG9hZC5waHA&ntb=1](https://www.bing.com/ck/a?!&&p=17025025dbeef5d650a67a3b4a49c0218bed35529ef61e6ddbfc38d974981ab8JmltdHM9MTczMTYyODgwMA&ptn=3&ver=2&hsh=4&fclid=29425e80-95ec-62a3-2119-51b5944e63b9&psq=%eb%b1%81%eb%af%b9%ec%8a%a4&u=a1aHR0cHM6Ly93d3cudmFwc2hpb24uY29tL3ZhcHNoaW9uMy9kb3dubG9hZC5waHA&ntb=1)](https://www.bing.com/ck/a?!&&p=17025025dbeef5d650a67a3b4a49c0218bed35529ef61e6ddbfc38d974981ab8JmltdHM9MTczMTYyODgwMA&ptn=3&ver=2&hsh=4&fclid=29425e80-95ec-62a3-2119-51b5944e63b9&psq=%eb%b1%81%eb%af%b9%ec%8a%a4&u=a1aHR0cHM6Ly93d3cudmFwc2hpb24uY29tL3ZhcHNoaW9uMy9kb3dubG9hZC5waHA&ntb=1)

[https://www.bing.com/ck/a?!&&p=17025025dbeef5d650a67a3b4a49c0218bed35529ef61e6ddbfc38d974981ab8JmltdHM9MTczMTYyODgwMA&ptn=3&ver=2&hsh=4&fclid=29425e80-95ec-62a3-2119-51b5944e63b9&psq=뱁믹스&u=a1aHR0cHM6Ly93d3cudmFwc2hpb24uY29tL3ZhcHNoaW9uMy9kb3dubG9hZC5waHA&ntb=1](https://www.bing.com/ck/a?!&&p=17025025dbeef5d650a67a3b4a49c0218bed35529ef61e6ddbfc38d974981ab8JmltdHM9MTczMTYyODgwMA&ptn=3&ver=2&hsh=4&fclid=29425e80-95ec-62a3-2119-51b5944e63b9&psq=%eb%b1%81%eb%af%b9%ec%8a%a4&u=a1aHR0cHM6Ly93d3cudmFwc2hpb24uY29tL3ZhcHNoaW9uMy9kb3dubG9hZC5waHA&ntb=1)

**darklabel.yaml 파일을 텍스트 형식이나 VScode 와 같은 개발 환경에서 열어
classes_set에 본인이 라벨링 할 객체 즉, 클래스들의 이름을 리스트 형식으로 작성해준다.**

(Open the darklabel.yaml file in a development environment such as text format or VScode in classes_set, the names of the objects to be labeled, that is, classes, are written in the form of a list.)

```python
classes_set: "my_classes1"   # predefined classes set (tag name of classes set)
```

```python
my_classes1:  [ "YogaBlock", "Kettlebell","MedBall","FoamRoller","LacrosseBall", "AquaBall"]
```

![화면 캡처 2024-11-16 000257.JPG](122d2ace-03b3-48b3-9d7c-ea7f66d602b7.png)

**VS 스튜디오에서 다음과 같이 코드를 입력하면  Annotation할 때 DarkLabel GUI에서 설정한 classes를 볼 수 있게 classes_set은 미리 설정해 놓은 ‘my_classes1’를 넣고 GUI에서 볼 name을 ’fitness_accessories’로 설정한다**

(In order to see the classes set in the DarkLabel GUI when announcing by entering the code as follows in VS Studio, class_set adds a preset 'my_classes1' and sets the name to be viewed in the GUI to 'fitness_accessories’)

![image.png](b7cf7ef5-a729-4acc-aead-eedcdfded97a.png)

```python
format1:    # darknet yolo (predefined format]
  fixed_filetype: 1                 # if specified as true, save setting isn't changeable in GUI
  data_fmt: [classid, ncx, ncy, nw, nh]
  gt_file_ext: "txt"                 # if not specified, default setting is used
  gt_merged: 0                    # if not specified, default setting is used
  delimiter: " "                     # if not spedified, default delimiter(',') is used
  classes_set: "my_classes1"     # if not specified, default setting is used
  name: "fitness_accessories"           # if not specified, "[fmt%d] $data_fmt" is used as default format name

```

![image.png](34721b64-c0de-4867-a483-203f7331ab20.png)

![image.png](9862f7a8-09bc-43cc-9975-e743580a2e20.png)

**다음과 같이 객체를 라벨링할 명칭들이 내가 설정한 클래스이름과 같은 것을 GUI에서 확인할 수 있다
(You can check on the GUI that the names to label objects are the same as the class names I set)**

**Open Video 를 눌러 내가 라벨링할 원본 영상을 불러온다(Press Open Video to sing the original image to label)**

# 영상을 이미지와 .txt파일로 추출하는 방법

---

**open video를 열어 추출할 영상을 선택 해준다.**

![image.png](2c470079-0ad1-468e-9a9c-24eca9a6ba05.png)

**영상을 열고 추출할 이미지를 저장할 폴더를 반드시 만들고 만든 폴더에 images 폴더를 더 만들어 innotation 된 .txt파일과 구별해준다.**
(Make sure you open the video and create a folder to store the image you want to extract and create more images folders in the folder you created to distinguish it from the notated .txt file.)

**Box + Label 을 체크해주고 각 소도구의 명칭에 맞게 라벨링을 해줍니다
라벨링시에는 Box + Label을 선택하여 라벨링시에 화면을 구분하는 용도이다.
(**Check the box + label and label it according to the name of each accessory
When labeling, Box + Label is selected to distinguish the screen during labeling.)

**라벨링을 모두 진행한 후 미리 저장한 폴더 images 폴더에 as Images 버튼을 눌러 이미지 형태로 저장해준다.**
(After all labeling, press the as images button in the pre-saved folder images folder to save it in the form of an image.)

![image.png](9cccd937-cd2f-4979-9b57-531b8345e1f4.png)

**이미지 형태로 저장을 완료한 후에 save as GT 버튼을 눌러 images 폴더와 구분되게 저장해준다.이런 구분 작업은 이후에 colab에 넣을 때 훨씬 더 편리하다.**
(After you complete the save in image format, press the save as GT button to save it distinct from the images folder. This distinction is much more convenient when you put it in the colab later.)

![image.png](5ecb932d-6a33-46de-a7e8-72dc0eea9159.png)

# Colab을 이용한 Yolov5 학습
(Learning Yolov5 with Colab)

---

### **1. Colab에 접속하여 및 구글 드라이브와 연결하여 하위 폴더 생성**
   (Connect to Colab and connect to Google Drive to create     
     subfolders)

```python
# prompt: 구글 드라이브랑 연결

from google.colab import drive
drive.flush_and_unmount()  # 기존 마운트 해제
drive.mount('/content/drive')  # 다시 마운트
```

```python
%cd /content/drive/MyDri
```

```python
# prompt: 구글 드라이브랑 연결

from google.colab import drive
drive.flush_and_unmount()  # 기존 마운트 해제
drive.mount('/content/drive')  # 다시 마운트
```

![image.png](61ecacb7-e44c-476e-9f82-a0299ed5368a.png)

---

### **2. YOLOv5 다운로드
(Download YOLOv5)**

```python
##
#clone YOLOv5 and
!git clone https://github.com/ultralytics/yolov5  # clone repo
%cd yolov5
%pip install -qr requirements.txt # install dependencies
```

```python
!pip install Pillow==10.3
```

 **YOLOv5에서 python을 쓰는데 Python 패키지들을 쉽게 설치, 관리할 수 있게 해주는 도구인 pip와 이미지 처리를 담당하는 Pillow라이브러리를 다운로드 한다.**
(YOLOv5 downloads pip, a tool that makes it easy to install and manage Python packages, and Pillow Library, which is responsible for image processing.)
****

![image.png](e3a7d8dd-aac2-486f-a2b8-152c041cb89e.png)

---

### **3. 이미지 파일들을 관리 폴더 생성**
(Create a management folder for image files)
****

```python
!mkdir -p Train/labels
!mkdir -p Train/images
!mkdir -p Val/labels
!mkdir -p Val/images
%pw
```

```python
##검증 데이터 만들기
import os
import shutil
from sklearn.model_selection import %pwdtrain_test_split

def create_validation_set(train_path, val_path, split_ratio=0.3):
    """
    Train 데이터의 일부를 Val로 이동
    """
    # 필요한 디렉토리 생성
    os.makedirs(os.path.join(val_path, 'images'), exist_ok=True)
    os.makedirs(os.path.join(val_path, 'labels'), exist_ok=True)

    # Train 이미지 리스트 가져오기
    train_images = os.listdir(os.path.join(train_path, 'images'))
    train_images = [f for f in train_images if f.endswith(('.jpg', '.jpeg', '.png'))]

    # Train/Val 분할
    _, val_images = train_test_split(train_images,
                                   test_size=split_ratio,
                                   random_state=42)

    # Val로 파일 복사
    for image_file in val_images:
        # 이미지 복사
        src_image = os.path.join(train_path, 'images', image_file)
        dst_image = os.path.join(val_path, 'images', image_file)
        shutil.copy2(src_image, dst_image)

        # 라벨 파일 복사
        label_file = os.path.splitext(image_file)[0] + '.txt'
        src_label = os.path.join(train_path, 'labels', label_file)
        dst_label = os.path.join(val_path, 'labels', label_file)
        if os.path.exists(src_label):
            shutil.copy2(src_label, dst_label)

    print(f"Created validation set with {len(val_images)} images")

# 실행
train_path = '/content/drive/MyDrive/yolov5/Train'
val_path = '/content/drive/MyDrive/yolov5/Val'

create_validation_set(train_path, val_path)
```

**Train 데이터에서 지정된 비율만큼 이미지를 추출해 Validation 데이터로 분리 및 복사하고 이미지와 함께 라벨 파일도 해당 경로로 복사하여 Validation 데이터셋을 구성한다. 해당코드의 기능은 다음과 같다.**
(The validation dataset is composed by extracting images from the training data at a specified rate, separating and copying them into validation data, and copying the label file along with the image to the corresponding path. The functions of the code are as follows.)
****

- **모델의 성능 평가(Evaluation of the performance of the model)**
    - Validation 데이터는 모델이 학습하지 않은 데이터로, 모델의 일반화 성능(새로운 데이터에 얼마나 잘 작동하는지를 평가하기 위해 필요
    (Validation data is data that the model has not learned, and is needed to evaluate the generalization performance of the model (how well it works on new data))
- **데이터 편향 방지(data bias prevention)**
    - 학습(Train) 데이터만 사용하면 모델이 데이터에 과적합(Overfitting)될 가능성이 높다.
    (Using only the training data is likely to overfit the model to the data.)
    - Validation 데이터를 통해 모델이 다양한 데이터에 대해 균형 잡힌 성능을 가지도록 확인할 수 있음
    (You can check the model to have balanced performance for various data)
- **하이퍼파라미터 튜닝(Hyperparameter tuning)**
    - 학습 도중 학습률, epoch 수, 모델 구조 등 하이퍼파라미터를 조정할 때 Validation 데이터의 결과를 참고하고 Validation 성능이 높아지는 방향으로 하이퍼파라미터를 최적화한다.
    (When adjusting hyperparameters such as learning rate, number of epochs, and model structure during learning, refer to the results of validation data and optimize the hyperparameters in the direction of increasing validation performance.)
    

**Validation 데이터를 분리하는 것은 모델의 성능 평가와 개선을 위한 필수 과정이다.**
(Isolation of validation data is an essential process for performance evaluation and improvement of the model.)

```python
def check_dataset():
    train_path = '/content/drive/MyDrive/yolov5/Train'
    val_path = '/content/drive/MyDrive/yolov5/Val'

    # Train 데이터 확인
    train_images = len(os.listdir(os.path.join(train_path, 'images')))
    train_labels = len(os.listdir(os.path.join(train_path, 'labels')))

    # Val 데이터 확인
    val_images = len(os.listdir(os.path.join(val_path, 'images')))
    val_labels = len(os.listdir(os.path.join(val_path, 'labels')))

    print("Dataset status:")
    print(f"Train - Images: {train_images}, Labels: {train_labels}")
    print(f"Val - Images: {val_images}, Labels: {val_labels}")

# 데이터셋 상태 확인
check_dataset()
```

![데이터셋의 크기가 다음과 같이 몇십에서 몇백장 처럼 큰 차이가없을 정도로 차이가 나면 이후에 학습시키는 데 큰 문제가 없다. 정확도가 미미하게 떨어지겠지만 현재 원본 이미지수에는 유의미할정도의 영향을 미치지 않음.
(If the size of the dataset differs to the extent that there is no significant difference between a few tens and a few hundred chapters as follows, there is no big problem in learning afterwards. The accuracy will be slightly lowered, but it does not significantly affect the current number of original images.)](295dd344-e5a3-4219-adb7-5f1338aa46e0.png)

데이터셋의 크기가 다음과 같이 몇십에서 몇백장 처럼 큰 차이가없을 정도로 차이가 나면 이후에 학습시키는 데 큰 문제가 없다. 정확도가 미미하게 떨어지겠지만 현재 원본 이미지수에는 유의미할정도의 영향을 미치지 않음.
(If the size of the dataset differs to the extent that there is no significant difference between a few tens and a few hundred chapters as follows, there is no big problem in learning afterwards. The accuracy will be slightly lowered, but it does not significantly affect the current number of original images.)

---

### **4. 학습시작
(Start learning)**

```python
import torch
import os
from IPython.display import Image, clear_output  # to display images
```

```python
%pwd
```

**import os :  운영체제와 상호작용하기 위한 OS**
(OS for interacting with the operating system)

**import torch** : **딥러닝 모델을 구축하고 학습시키기 위해 사용되고 
딥러닝 모델 학습, 추론, 텐서 연산 등에 활용되는 pytorch라이브러리**

(It is used to build and train deep learning models
Pytorch library used for deep learning model training, inference, and tensor operations)

```python
import numpy as np
import tensorflow as tf
import os
from PIL import Image
from tensorflow.python.eager.context import eager_mode

def _preproc(image, output_height=512, output_width=512, resize_side=512):
    ''' imagenet-standard: aspect-preserving resize to 256px smaller-side, then central-crop to 224px'''
    with eager_mode():
        h, w = image.shape[0], image.shape[1]
        scale = tf.cond(tf.less(h, w), lambda: resize_side / h, lambda: resize_side / w)
        resized_image = tf.compat.v1.image.resize_bilinear(tf.expand_dims(image, 0), [int(h*scale), int(w*scale)])
        cropped_image = tf.compat.v1.image.resize_with_crop_or_pad(resized_image, output_height, output_width)
        return tf.squeeze(cropped_image)

def Create_npy(imagespath, imgsize, ext) :
    images_list = [img_name for img_name in os.listdir(imagespath) if
                os.path.splitext(img_name)[1].lower() == '.'+ext.lower()]
    calib_dataset = np.zeros((len(images_list), imgsize, imgsize, 3), dtype=np.float32)

    for idx, img_name in enumerate(sorted(images_list)):
        img_path = os.path.join(imagespath, img_name)
        try:
            # 파일 크기가 정상적인지 확인
            if os.path.getsize(img_path) == 0:
                print(f"Error: {img_path} is empty.")
                continue

            img = Image.open(img_path)
            img = img.convert("RGB")  # RGBA 이미지 등 다른 형식이 있을 경우 강제로 RGB로 변환
            img_np = np.array(img)

            img_preproc = _preproc(img_np, imgsize, imgsize, imgsize)
            calib_dataset[idx,:,:,:] = img_preproc.numpy().astype(np.uint8)
            print(f"Processed image {img_path}")

        except Exception as e:
            print(f"Error processing image {img_path}: {e}")

    np.save('calib_set.npy', calib_dataset)
```

**이 코드는 지정된 폴더의 이미지를 읽어들여,비율 유지 리사이즈와 
중앙 크롭/패딩으로 전처리한 후 Numpy 배열로 저장한다.**

**전처리된 데이터는**.npy**파일로 저장되어 모델 학습 또는 교정 데이터로 활용된다.**

(This code reads the image of the specified folder, contains the ratio maintenance resize and Pretreatment with center crop/padding and save in a Numpy array. The preprocessed data is stored as a .npy file and used as model training or calibration data.)

```python
#모델 학습하기
!python  /content/drive/MyDrive/yolov5/train.py  --img 640 --batch 16 --epochs 300 --data /content/drive/MyDrive/yolov5/data.yaml --weights yolov5n.pt --cache --patience 0  

```

**YOLOv5를 실행시키기위한 
`train.py`를 실행한다. 다양한 옵션이 포함되어 있는데 다음과 같다.**

(YOLOv5를 실행시키기위한
train.py를 실행한다. 다양한 옵션이 포함되어 있는데 다음과 같다.)

- **`--img 640`:**
    
    **입력 이미지의 크기를  640x640으로 설정한다.(**Set the size of the input image to 640x640.)
    
- **`--batch 16`:**
    
     **배치 크기를 설정한다. 한 번에 처리되는 이미지의 수를 나타낸다.
    (**Set the batch size. Indicates the number of images processed at a time.)
    
- **`--epochs 300`:**
    
     **학습 반복 횟수 에폭(epoch) 수를 설정합니다.
    (** Sets the total number of epochs to learn.)
    
- **`--data /content/drive/MyDrive/yolov5/data.yaml`**
    
    **데이터셋 및 모델 구성에 대한 설정이 담긴 YAML 파일의 경로를 지정한다.**
    Specify the path to the YAML file containing settings for the dataset and model configurations.)
    
- **`--weights yolov5n.pt`:**
    
    **미리 훈련된 가중치 파일의 경로를 지정한다. yolov5에서는 `yolov5n.pt` 파일을 사용하고 있다.
    (** Specify the path to pre-trained weight files. **`yolov5n.pt `** files are being used here.)
    
- **`--patience 0` :**
    
    **학습이 중간에 멈추는걸 방지해준다. 10으로 설정하면 10동안 epoch가 변화가 없으면 정지한다.**(It prevents learning from stopping in the middle. Set to 10, if the epoch remains unchanged for 10 years, it stops.)
    
    ---
    
    ![image.png](2367191c-4540-4cb0-91b4-5c407216b1c6.png)
    
    **학습이 완료되면 yolov5/runs/train/exp10 의 경로에 결과가 저장되었다는 문구를 볼 수 있다.**
    
    ---
    
    ### **5.  yolov5모델 학습 결과 검증**
    
    ```python
    %load_ext tensorboard
    %tensorboard --logdir /content/drive/MyDrive/yolov5/runs/train/exp10
    ```
    
    **위의 코드를 이용하여 --logdir /content/drive/MyDrive/yolov5/runs/train/exp10와 같이 학습한 경로를라벨리 지정하고 텐서보드를 이용하여 시각화 할 수 있다.**
    

![image.png](476d1e50-27e1-47b8-939b-084be7098f62.png)

**학습한 자료들을 바탕으로 라벨링된 해당 객체가 다른 영상이나 이미지에 나오는 것을 재학습하여 검증하는 코드는 다음과 같다.** 
(The code for re-learning and verifying that the object labeled based on the learned materials appears in other images or images is as follows.)

```python
!python /content/drive/MyDrive/yolov5/detect.py --weights /content/drive/MyDrive/yolov5/runs/train/exp4/weights/best.pt --img 640 --conf 0.1 --source /content/drive/MyDrive/yolov5/Train/images
```

```python
!python /content/drive/MyDrive/yolov5/detect.py --weights /content/drive/MyDrive/yolov5/runs/train/exp5/weights/best.pt --img 640 --conf 0.1 --source /content/drive/MyDrive/yolov5/Train/라있.mp4
```

# **학습 결과물
(The results of learning)**

**학습 돌린 라벨 결과물 train_batch**

![F1_curve.png](F1_curve.png)

![labels.jpg](labels.jpg)

![confusion_matrix.png](confusion_matrix.png)

![labels_correlogram.jpg](labels_correlogram.jpg)

![PR_curve.png](PR_curve.png)

![P_curve.png](P_curve.png)

![R_curve.png](R_curve.png)

![results.png](results.png)

![val_batch0_labels.jpg](val_batch0_labels.jpg)

![val_batch2_labels.jpg](val_batch2_labels.jpg)

![val_batch2_pred.jpg](val_batch2_pred.jpg)

![image.png](image.png)

# **학습결과 영상
(Learning Results Video)**

[https://youtu.be/yWuiWQbhLDA?si=vcco28nqa3DvEiR3](https://youtu.be/yWuiWQbhLDA?si=vcco28nqa3DvEiR3)

---

[https://youtube.com/shorts/7mulq1dcq00?si=3GKYVtTEMI6OMfBT](https://youtube.com/shorts/7mulq1dcq00?si=3GKYVtTEMI6OMfBT)

---

[https://youtu.be/xB4_d9nBZXw?si=Lv8a9Qi-PQ2SNWN4](https://youtu.be/xB4_d9nBZXw?si=Lv8a9Qi-PQ2SNWN4)

---

[https://youtube.com/shorts/riOYFq0lBrA?si=aCtcZegWiGhgOptZ](https://youtube.com/shorts/riOYFq0lBrA?si=aCtcZegWiGhgOptZ)

---

[https://youtu.be/31t7jyiUw3I?si=IIRwK7cRcrfU0K1E](https://youtu.be/31t7jyiUw3I?si=IIRwK7cRcrfU0K1E)

---

[https://youtu.be/50bJGmlF54Q?si=KXy3Aq4l-5f_SBgm](https://youtu.be/50bJGmlF54Q?si=KXy3Aq4l-5f_SBgm)

[https://drive.google.com/drive/folders/1eQhV7VjYvbUjdPRBkt-Feh6n9A50mIaH?usp=drive_link](https://drive.google.com/drive/folders/1eQhV7VjYvbUjdPRBkt-Feh6n9A50mIaH?usp=drive_link)